{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3bc166c",
   "metadata": {},
   "source": [
    "# üß† Machine Learning Web App (SAIR Project)\n",
    "\n",
    "This project is part of the **SAIR (Sudanese Artificial Intelligence Road)** community learning path.  \n",
    "After completing the Regression and Classification modules, this project focuses on applying that knowledge in a practical way.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Goal\n",
    "The goal is to build an **interactive web application** that allows users to perform common machine learning tasks without writing code.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è What the App Does\n",
    "- üóÇÔ∏è **Upload Dataset:** Users can upload their own dataset (CSV or similar).  \n",
    "- üìä **Select Problem Type:** Choose whether the dataset is for **regression** or **classification**.  \n",
    "- ü§ñ **Choose or Upload Model:** Users can either:\n",
    "  - Upload a pre-trained model, **or**\n",
    "  - Select a model from the provided list (e.g., Linear Regression, Logistic Regression, Random Forest, etc.).\n",
    "- üîÅ **Full Pipeline Mode:** The app can automatically:\n",
    "  - Preprocess the data  \n",
    "  - Split into train/test sets  \n",
    "  - Train the model  \n",
    "  - Evaluate its performance  \n",
    "  - Show predictions and metrics visually  \n",
    "\n",
    "---\n",
    "\n",
    "## üí° Purpose\n",
    "To make machine learning **simple, fast, and accessible** ‚Äî allowing anyone to test and train models through an intuitive web interface.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5cd9e2",
   "metadata": {},
   "source": [
    "    \n",
    "    \n",
    "    \n",
    "    üõ†Ô∏è Production Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92fbad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14757578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Advanced imports for production ML\n",
    "\n",
    "# Suppress warnings for cleaner outputs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# üîß Core Python libraries\n",
    "import numpy as np           # Efficient numerical computations\n",
    "import pandas as pd          # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # Basic plotting\n",
    "import seaborn as sns        # Advanced visualization\n",
    "from scipy import stats      # Statistical functions\n",
    "import joblib               # Save/load large models and preprocessing objects\n",
    "import json                 # Handle JSON configs and outputs\n",
    "from datetime import datetime  # Timestamping for logs\n",
    "import os                   # File system operations\n",
    "import time                 # Time tracking for experiments\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "# üß∞ Sklearn libraries - expanded for advanced ML workflows\n",
    "from sklearn.datasets import fetch_california_housing  # Real-world dataset\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,     # Split data into train/test sets\n",
    "    cross_val_score,      # Cross-validation scoring\n",
    "    GridSearchCV,         # Hyperparameter tuning (grid search)\n",
    "    RandomizedSearchCV    # Hyperparameter tuning (randomized search)\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,       # Feature scaling (zero-mean, unit variance)\n",
    "    RobustScaler,         # Scaling robust to outliers\n",
    "    PolynomialFeatures    # Generate polynomial features for non-linear relationships\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion  # Build modular pipelines\n",
    "from sklearn.compose import ColumnTransformer         # Apply different preprocessing to columns\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,          # Univariate feature selection\n",
    "    f_regression,         # Scoring function for regression\n",
    "    RFE                   # Recursive feature elimination\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression,     # Baseline regression\n",
    "    Ridge,                # L2-regularized regression\n",
    "    Lasso,                # L1-regularized regression\n",
    "    ElasticNet            # Combination of L1 and L2 regularization\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,       # Ensemble of decision trees\n",
    "    GradientBoostingRegressor,   # Boosted trees for regression\n",
    "    VotingRegressor              # Combine multiple regressors\n",
    ")\n",
    "from sklearn.svm import SVR               # Support Vector Regression\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,  # Regression metric\n",
    "    r2_score,            # Regression metric\n",
    "    mean_absolute_error  # Regression metric\n",
    ")\n",
    "from sklearn.inspection import (\n",
    "    permutation_importance,       # Feature importance\n",
    "    PartialDependenceDisplay      # Partial dependence plots\n",
    ")\n",
    "\n",
    "# üß™ Advanced model tracking with MLflow\n",
    "import mlflow                  # Experiment tracking\n",
    "import mlflow.sklearn          # Log sklearn models\n",
    "from mlflow.models.signature import infer_signature  # Auto-capture input/output schema for reproducible deployment\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier # <--- New Import\n",
    "from lightgbm import LGBMClassifier # <--- New Import\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea04fc9",
   "metadata": {},
   "source": [
    "# üß† Getting data from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "394d662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/abdelhadi/SAIR_2_project/experiments/371399585566611001', creation_time=1763800319038, experiment_id='371399585566611001', last_update_time=1763800319038, lifecycle_stage='active', name='AutoML_Benchmark', tags={}>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config:\n",
    "    # Reproducibility - Critical for production!\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    VAL_SIZE = 0.2  # NEW: Validation set for tuning\n",
    "    CV_FOLDS = 5\n",
    "    N_JOBS = -1  # Use all available cores\n",
    "    \n",
    "    # Model directories - Organized project structure\n",
    "    MODEL_DIR = \"models\"\n",
    "    EXPERIMENT_DIR = \"experiments\"\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "# Initialize MLflow for experiment tracking\n",
    "mlflow.set_tracking_uri(f\"file://{os.path.abspath(config.EXPERIMENT_DIR)}\")\n",
    "experiment_name = \"AutoML_Benchmark\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b8701f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Function 1: Load and Store Data ---\n",
    "\n",
    "\n",
    "# This is tied to the first button.\n",
    "# It returns the dataframe twice: once to see, once to save.\n",
    "def load_and_store_data(file):\n",
    "    if file is None: return None, None\n",
    "    try:\n",
    "        df = pd.read_csv(file.name)\n",
    "        return df, df \n",
    "    except Exception as e:\n",
    "        return None, None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7b423d",
   "metadata": {},
   "source": [
    "## üîç Step 2: Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "caff7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. EDA Functions ---\n",
    "def find_missing_values(df):\n",
    "    if df is None: return None\n",
    "    missing = df.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "    if missing.empty:\n",
    "        return pd.DataFrame(columns=[\"Column\", \"Missing Count\"])\n",
    "    missing_df = missing.to_frame(name=\"Missing Count\").reset_index().rename(columns={'index': 'Column'})\n",
    "    return missing_df\n",
    "\n",
    "def find_categorical_columns(df):\n",
    "    if df is None: return None\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(cat_cols) == 0:\n",
    "        return pd.DataFrame(columns=[\"Categorical Columns\"])\n",
    "    return pd.DataFrame(cat_cols, columns=[\"Categorical Columns\"])\n",
    "\n",
    "def get_shape(df):\n",
    "    if df is None: return \"No data loaded.\"\n",
    "    rows, cols = df.shape\n",
    "    return f\"Dataset has {rows} rows and {cols} columns.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a2997",
   "metadata": {},
   "source": [
    "# --- 3. Split X/y Function ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68a38ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Split X/y Function ---\n",
    "def split_and_save_data(df, y_column_name):\n",
    "    if df is None:\n",
    "        return \"Please upload a file first.\", None, None, None, None\n",
    "    if not y_column_name:\n",
    "        return \"Please enter a 'y' column name.\", None, None, None, None\n",
    "    if y_column_name not in df.columns:\n",
    "        return f\"Error: '{y_column_name}' is not in the dataset.\", None, None, None, None\n",
    "    \n",
    "    df_cleaned = df.dropna(subset=[y_column_name])\n",
    "    y = df_cleaned[[y_column_name]].astype(int)\n",
    "    X = df_cleaned.drop(columns=[y_column_name])\n",
    "    \n",
    "    return f\"Success! Split '{y_column_name}'. Data is ready for preprocessing.\", X, y, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d761a884",
   "metadata": {},
   "source": [
    "# preproscessing the dataset (fixing the missing and categorical columns ) and spliting to val and train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c10bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. Preprocessing Function (Matches your latest code) ---\n",
    "def preprocess_data(X, y):\n",
    "    if X is None or y is None:\n",
    "        return \"Please split your data in Tab 3 first.\", None, None, None, None, None, None, None, None\n",
    "\n",
    "    try:\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "        \n",
    "        missing_numeric_cols = X_train.select_dtypes(include=np.number).isnull().sum()\n",
    "        missing_numeric_cols = missing_numeric_cols[missing_numeric_cols > 0].index.tolist()\n",
    "        categorical_cols_to_fix = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "        numeric_features = X.select_dtypes(include=np.number).columns\n",
    "        categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "        numeric_transformer = SimpleImputer(strategy='median')\n",
    "        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "\n",
    "        preprocessor.fit(X_train)\n",
    "        \n",
    "        X_train_processed = preprocessor.transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "        X_test_processed = preprocessor.transform(X_test)\n",
    "        \n",
    "        status = (\n",
    "            f\"Preprocessing complete!\\n\"\n",
    "            f\"Data split (Train/Val/Test): {len(X_train)} / {len(X_val)} / {len(X_test)}\\n\"\n",
    "            f\"Processed X_train shape: {X_train_processed.shape}\\n\\n\"\n",
    "            f\"--- Columns Fixed ---\\n\"\n",
    "            f\"Imputed (filled missing): {missing_numeric_cols}\\n\"\n",
    "            f\"Encoded (text to numbers): {categorical_cols_to_fix}\"\n",
    "        )\n",
    "        \n",
    "        # 5. Fix the Sparse Matrix Issue\n",
    "        # Convert sparse matrix to dense array so pandas can read it\n",
    "        if hasattr(X_train_processed, \"toarray\"):\n",
    "            X_train_processed = X_train_processed.toarray()\n",
    "            X_val_processed = X_val_processed.toarray()\n",
    "            X_test_processed = X_test_processed.toarray()\n",
    "        # Get new column names\n",
    "        new_cols = preprocessor.get_feature_names_out()\n",
    "        \n",
    "        # Create full DataFrame for preview and plotting\n",
    "        X_train_df = pd.DataFrame(X_train_processed, columns=new_cols)\n",
    "        \n",
    "        # For preview (first 5 rows)\n",
    "        preview_df = X_train_df.head()\n",
    "\n",
    "        # Return 9 items\n",
    "        return (status, preview_df, \n",
    "                X_train_processed, y_train, \n",
    "                X_val_processed, y_val, \n",
    "                X_test_processed, y_test,\n",
    "                X_train_df) # <--- 9th Item: The DataFrame for plotting\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\", None, None, None, None, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb79e0",
   "metadata": {},
   "source": [
    "## üîç Graphs of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c65dc14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Plotting Functions (Using X_train_df) ---\n",
    "def plot_all_distributions(X_train_df):\n",
    "    if X_train_df is None:\n",
    "        return None\n",
    "    try:\n",
    "        cols = X_train_df.columns.tolist()\n",
    "        num_cols = len(cols)\n",
    "        grid_size = int(np.ceil(np.sqrt(num_cols)))\n",
    "        \n",
    "        fig, axes = plt.subplots(grid_size, grid_size, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(cols):\n",
    "            sns.histplot(X_train_df[col], kde=True, ax=axes[i])\n",
    "            axes[i].set_title(col, fontsize=10)\n",
    "        \n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def plot_correlation(X_train_df):\n",
    "    if X_train_df is None:\n",
    "        return None\n",
    "    try:\n",
    "        fig = plt.figure(figsize=(12, 10))\n",
    "        corr = X_train_df.corr(numeric_only=True) \n",
    "        sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "        \n",
    "          \n",
    "        plt.title(\"Correlation Heatmap\")\n",
    "        \n",
    "        \n",
    "        return fig\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eea0e3",
   "metadata": {},
   "source": [
    "# get form the user the type of the dataset \"regression or classification\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c13d231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper to save the task type ---\n",
    "def update_task_type(new_value):\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcacface",
   "metadata": {},
   "source": [
    "# outlierhandeler and the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa4dc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NEW: Outlier Handler for Robust Models\n",
    "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Handle outliers using IQR method - More robust than simple scaling\"\"\"\n",
    "    \n",
    "    def __init__(self, factor=1.5):\n",
    "        self.factor = factor\n",
    "        self.lower_bounds_ = None\n",
    "        self.upper_bounds_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.lower_bounds_ = []\n",
    "        self.upper_bounds_ = []\n",
    "        \n",
    "        # Calculate IQR bounds for each feature\n",
    "        for i in range(X.shape[1]):\n",
    "            Q1 = np.percentile(X[:, i], 25)  # 25th percentile\n",
    "            Q3 = np.percentile(X[:, i], 75)  # 75th percentile  \n",
    "            IQR = Q3 - Q1  # Interquartile Range\n",
    "            self.lower_bounds_.append(Q1 - self.factor * IQR)\n",
    "            self.upper_bounds_.append(Q3 + self.factor * IQR)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        # Clip values to IQR bounds\n",
    "        for i in range(X.shape[1]):\n",
    "            lower = self.lower_bounds_[i]\n",
    "            upper = self.upper_bounds_[i]\n",
    "            X_transformed[:, i] = np.clip(X_transformed[:, i], lower, upper)\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create comprehensive preprocessing pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('outlier_handler', OutlierHandler(factor=1.5)),  # Handle outliers\n",
    "    ('scaler', RobustScaler())  # Robust to outliers (better than StandardScaler)\n",
    "])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a136f4",
   "metadata": {},
   "source": [
    "# using the preprocessor pipeline in the ui \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9db3330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scale_and_clean_data(X_train_processed, X_val_processed, X_test_processed, task_type):\n",
    "    # 1. Safety Check\n",
    "    if X_train_processed is None: \n",
    "        return \"Please run Stage 1 (Preprocessing) first.\", None, None, None\n",
    "\n",
    "    try:\n",
    "        # --- REGRESSION LOGIC ---\n",
    "        if task_type == \"Regression\":\n",
    "            # 2. Create the Advanced Pipeline\n",
    "            advanced_pipeline = Pipeline([\n",
    "                ('outlier_handler', OutlierHandler(factor=1.5)),  # Clip Outliers\n",
    "                ('scaler', RobustScaler())  # Scale Robustly\n",
    "            ])\n",
    "            \n",
    "            # 3. Fit on Training Data Only\n",
    "            advanced_pipeline.fit(X_train_processed)\n",
    "            \n",
    "            # 4. Transform All Sets\n",
    "            X_train_scaled = advanced_pipeline.transform(X_train_processed)\n",
    "            X_val_scaled = advanced_pipeline.transform(X_val_processed)\n",
    "            X_test_scaled = advanced_pipeline.transform(X_test_processed)\n",
    "            \n",
    "            status = (f\"‚úÖ Advanced Processing Complete (Regression)\\n\"\n",
    "                      f\"‚Ä¢ Outliers Capped (Winsorized)\\n\"\n",
    "                      f\"‚Ä¢ Data Scaled (RobustScaler)\")\n",
    "            \n",
    "            return status, X_train_scaled, X_val_scaled, X_test_scaled\n",
    "\n",
    "        # --- CLASSIFICATION LOGIC ---\n",
    "        elif task_type == \"Classification\":\n",
    "\n",
    "            # 5. scaling the data \n",
    "            advanced_pipeline = Pipeline([('scaler', RobustScaler()) ])\n",
    "             # 3. Fit on Training Data Only\n",
    "            advanced_pipeline.fit(X_train_processed)\n",
    "             # 4. Transform All Sets\n",
    "            X_train_scaled = advanced_pipeline.transform(X_train_processed)\n",
    "            X_val_scaled = advanced_pipeline.transform(X_val_processed)\n",
    "            X_test_scaled = advanced_pipeline.transform(X_test_processed)\n",
    "            status = (f\"‚úÖ Advanced Processing Complete (classification)\\n\"\n",
    "                      f\"‚Ä¢ Data Scaled (RobustScaler)\")\n",
    "            # Just return the data as-is for now\n",
    "            return status, X_train_scaled, X_val_scaled, X_test_scaled\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad29f49",
   "metadata": {},
   "source": [
    "# over viwe of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72ae40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# --- 2.5 Profiling Function ---\n",
    "def generate_profile_report(df):\n",
    "    if df is None:\n",
    "        return \"Please upload a file in Tab 1 first.\"\n",
    "    \n",
    "    try:\n",
    "        # minimal=True is CRITICAL for web apps (makes it fast)\n",
    "        profile = ProfileReport(df, title=\"Dataset Profiling Report\", minimal=False)\n",
    "        return profile.to_html()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error generating report: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefe453",
   "metadata": {},
   "source": [
    "# fucntion for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9f1bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Model Portfolio (Global) ---\n",
    "advanced_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=config.RANDOM_STATE),\n",
    "    'Lasso': Lasso(random_state=config.RANDOM_STATE),\n",
    "    'ElasticNet': ElasticNet(random_state=config.RANDOM_STATE),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=config.RANDOM_STATE, n_jobs=config.N_JOBS),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=config.RANDOM_STATE),\n",
    "    'SVR': SVR()\n",
    "}\n",
    "\n",
    "# Add Ensemble (depends on the others)\n",
    "voting_ensemble = VotingRegressor([\n",
    "    ('ridge', Ridge(random_state=config.RANDOM_STATE)),\n",
    "    ('rf', RandomForestRegressor(random_state=config.RANDOM_STATE, n_jobs=config.N_JOBS)),\n",
    "    ('gb', GradientBoostingRegressor(random_state=config.RANDOM_STATE))\n",
    "])\n",
    "advanced_models['Voting Ensemble'] = voting_ensemble\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model_advanced(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    try:\n",
    "        # 1. Train\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # 2. Predict (Validation - For Ranking)\n",
    "        val_preds = model.predict(X_val)\n",
    "        val_r2 = r2_score(y_val, val_preds)\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "        \n",
    "        # 3. Predict (Training - For Diagnostics ONLY)\n",
    "        train_preds = model.predict(X_train)\n",
    "        train_r2 = r2_score(y_train, train_preds)\n",
    "        \n",
    "        # 4. Calculate Overfitting Gap\n",
    "        gap = train_r2 - val_r2\n",
    "        \n",
    "        # 5. Cross-Validation (Stability Check)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=config.CV_FOLDS, scoring='r2', n_jobs=config.N_JOBS)\n",
    "        cv_mean = cv_scores.mean()\n",
    "        \n",
    "        # 6. MLflow Logging\n",
    "        with mlflow.start_run(run_name=f\"Train_{model_name}\", nested=True):\n",
    "            mlflow.log_param(\"model_name\", model_name)\n",
    "            mlflow.log_metric(\"val_r2\", val_r2)\n",
    "            mlflow.log_metric(\"train_r2\", train_r2) # Log training score too\n",
    "            mlflow.log_metric(\"overfitting_gap\", gap)\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        # 7. Create the Plot\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        sns.scatterplot(x=y_val, y=val_preds, alpha=0.6, ax=ax)\n",
    "        min_v, max_v = y_val.min(), y_val.max()\n",
    "        ax.plot([min_v, max_v], [min_v, max_v], 'r--', lw=2)\n",
    "        ax.set_title(f\"{model_name} (Val R¬≤: {val_r2:.2f})\", fontsize=10)\n",
    "        \n",
    "        metrics = {\n",
    "            \"Model\": model_name,\n",
    "            \"Val R¬≤\": round(val_r2, 4),\n",
    "            \"Train R¬≤\": round(train_r2, 4),   # Added this\n",
    "            \"Overfitting\": round(gap, 4),     # Added this\n",
    "            \"CV R¬≤\": round(cv_mean, 4),\n",
    "            \"Time (s)\": round(duration, 2)\n",
    "        }\n",
    "        return metrics, fig, model, val_r2\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name}: {e}\")\n",
    "        return None, None, None, -np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77814129",
   "metadata": {},
   "source": [
    "# fuction for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18f66f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#--- CLASSIFICATION models  (Updated) ---\n",
    "classification_models = {\n",
    "    # üìä Linear Models\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        max_iter=1000,\n",
    "        C=0.1,\n",
    "        solver='liblinear'\n",
    "    ),\n",
    "\n",
    "    # üå≤ Tree-based Methods\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=config.RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        n_jobs=config.N_JOBS,\n",
    "        n_estimators=200,\n",
    "        max_depth=15\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1\n",
    "    ),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=config.RANDOM_STATE),\n",
    "\n",
    "    # üöÄ Advanced Boosters (NEW)\n",
    "    'XGBoost': XGBClassifier(\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        n_jobs=config.N_JOBS,\n",
    "        eval_metric='logloss',\n",
    "        n_estimators=200, # Reduced slightly for speed in web app\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8\n",
    "    ),\n",
    "    \n",
    "    'LightGBM': LGBMClassifier(\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        n_jobs=config.N_JOBS,\n",
    "        verbose=-1,\n",
    "        n_estimators=200, # Reduced slightly for speed\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31\n",
    "    ),\n",
    "\n",
    "    # üîç Instance-based Methods\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_jobs=config.N_JOBS),\n",
    "\n",
    "    # üéØ Kernel Methods\n",
    "    'Support Vector Machine': SVC(\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        probability=True,\n",
    "        kernel='rbf',\n",
    "        C=1.0\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# classification traning fuction\n",
    "\n",
    "def evaluate_classification_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    try:\n",
    "        # 1. Train\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # 2. Predict\n",
    "        val_preds = model.predict(X_val)\n",
    "        train_preds = model.predict(X_train)\n",
    "        \n",
    "        # 3. Metrics (CLASSIFICATION LOGIC)\n",
    "        # Accuracy: How many did we get right?\n",
    "        val_acc = accuracy_score(y_val, val_preds)\n",
    "        train_acc = accuracy_score(y_train, train_preds)\n",
    "        \n",
    "        # F1 Score: Better for imbalanced data (weighted average)\n",
    "        val_f1 = f1_score(y_val, val_preds, average='weighted')\n",
    "        \n",
    "        # 4. Calculate Overfitting Gap\n",
    "        gap = train_acc - val_acc\n",
    "        \n",
    "        # 5. Cross-Validation (Stability Check)\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, \n",
    "                                  cv=config.CV_FOLDS, scoring='accuracy', n_jobs=config.N_JOBS)\n",
    "        cv_mean = cv_scores.mean()\n",
    "        \n",
    "        # 6. MLflow Logging\n",
    "        with mlflow.start_run(run_name=f\"Train_{model_name}\", nested=True):\n",
    "            mlflow.log_param(\"model_name\", model_name)\n",
    "            mlflow.log_metric(\"val_accuracy\", val_acc)\n",
    "            mlflow.log_metric(\"val_f1\", val_f1)\n",
    "            mlflow.log_metric(\"overfitting_gap\", gap)\n",
    "            mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        # 7. Plot: Confusion Matrix (Not Scatter Plot!)\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        cm = confusion_matrix(y_val, val_preds)\n",
    "        \n",
    "        # Heatmap showing True vs Predicted counts\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        \n",
    "        ax.set_title(f\"{model_name} (Acc: {val_acc:.2%})\", fontsize=10)\n",
    "        ax.set_xlabel(\"Predicted Label\")\n",
    "        ax.set_ylabel(\"True Label\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        metrics = {\n",
    "            \"Model\": model_name,\n",
    "            \"Val Accuracy\": round(val_acc, 4),\n",
    "            \"Train Accuracy\": round(train_acc, 4),\n",
    "            \"Overfitting\": round(gap, 4),\n",
    "            \"CV Accuracy\": round(cv_mean, 4),\n",
    "            \"F1 Score\": round(val_f1, 4),\n",
    "            \"Time (s)\": round(duration, 2)\n",
    "        }\n",
    "        return metrics, fig, model, val_acc\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training {model_name}: {e}\")\n",
    "        return None, None, None, -np.inf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bc4a57",
   "metadata": {},
   "source": [
    "# for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd94d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Auto-ML Pipeline Function (Fixed) ---\n",
    "def train_models_pipeline(X_train_final, y_train, X_val_final, y_val,X_test_final,y_test, task_type):\n",
    "    # 1. Safety Checks\n",
    "    if X_train_final is None or y_train is None:\n",
    "        return None, None, None, \"Please run Advanced Processing (Tab 5) first.\"\n",
    "    \n",
    "    if isinstance(y_train, (pd.DataFrame, pd.Series)):\n",
    "        y_train = y_train.values.ravel()\n",
    "    else:\n",
    "        y_train = np.ravel(y_train)\n",
    "        \n",
    "    if isinstance(y_val, (pd.DataFrame, pd.Series)):\n",
    "        y_val = y_val.values.ravel()\n",
    "    else:\n",
    "        y_val = np.ravel(y_val)\n",
    "    y_test = np.ravel(y_test)\n",
    "    \n",
    "    # 2. Setup\n",
    "    mlflow.set_experiment(\"AutoML_Benchmark\")\n",
    "    results_list = []\n",
    "    best_score = -np.inf\n",
    "    best_model = None\n",
    "    best_model_name = \"\"\n",
    "    \n",
    "    ## ==========================\n",
    "    # REGRESSION PIPELINE\n",
    "    # ==========================\n",
    "    if task_type == \"Regression\":\n",
    "        \n",
    "        # 3. Loop through Global Model Portfolio\n",
    "        for name, model in advanced_models.items():\n",
    "            try:\n",
    "                # A. Run the Worker Function\n",
    "                metrics, _, trained_model, score = evaluate_model_advanced(\n",
    "                    model, X_train_final, y_train, X_val_final, y_val, name\n",
    "                )\n",
    "                \n",
    "                if metrics:\n",
    "                    results_list.append(metrics)\n",
    "                    \n",
    "                    # B. Track the Winner\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_model = trained_model\n",
    "                        best_model_name = name\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {name} due to error: {e}\")\n",
    "                continue\n",
    "\n",
    "        # --- CRITICAL FIX: Check if we actually trained anything ---\n",
    "        if len(results_list) == 0:\n",
    "            return None, None, None, \"Error: No models were trained successfully. Check your data shape.\"\n",
    "\n",
    "        # 4. Create Leaderboard DataFrame\n",
    "        leaderboard = pd.DataFrame(results_list).sort_values(by=\"Val R¬≤\", ascending=False)\n",
    "\n",
    "        # 5. Create Bar Chart (The Only Plot)\n",
    "        try:\n",
    "            fig = plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(data=leaderboard, x='Val R¬≤', y='Model', palette='viridis')\n",
    "            \n",
    "            plt.title('Model Comparison: Validation R¬≤ (Higher is Better)', fontsize=14)\n",
    "            plt.xlabel('R¬≤ Score')\n",
    "            plt.ylabel('')\n",
    "            plt.xlim(0, 1) \n",
    "            plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "        except Exception as e:\n",
    "            print(f\"Plotting error: {e}\")\n",
    "            fig = None\n",
    "        \n",
    "        # 6. Save Best Model\n",
    "        best_model_path = \"best_model.pkl\"\n",
    "        if best_model:\n",
    "            joblib.dump(best_model, best_model_path)\n",
    "        \n",
    "\n",
    "        # 6. TEST Scores (Final \"Real World\" Check) <-- NEW STEP\n",
    "        test_preds = best_model.predict(X_test_final)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "        test_r2 = r2_score(y_test, test_preds)\n",
    "        status = (f\"‚úÖ Training Complete!\\n\"\n",
    "                  f\"üèÜ Best Model: {best_model_name}\\n\"\n",
    "                  f\"üìà Best R¬≤: {best_score:.4f}\\n\"\n",
    "                  f\"üöÄ FINAL TEST SCORES (Held-out Data):\\n\"\n",
    "                  f\"‚Ä¢ R¬≤ ScX_train_processed, X_val_processed, X_test_processedore: {test_r2:.4f}\\n\"\n",
    "                  f\"‚Ä¢ RMSE:     {test_rmse:.4f}\\n\"\n",
    "                  f\"üíæ Saved to: {best_model_path}\")\n",
    "        \n",
    "        # Return: Leaderboard, Bar Plot, File, Status\n",
    "        return leaderboard, fig, best_model_path, status\n",
    "\n",
    "    # ==========================\n",
    "    # CLASSIFICATION PIPELINE\n",
    "    # ==========================\n",
    "    elif task_type == \"Classification\":\n",
    "        for name, model in classification_models.items(): # Using classification_models dictionary\n",
    "            try:\n",
    "                # Use the Classification Worker\n",
    "                metrics, _, trained_model, score = evaluate_classification_model(\n",
    "                    model, X_train_final, y_train, X_val_final, y_val, name\n",
    "                )\n",
    "                if metrics:\n",
    "                    results_list.append(metrics)\n",
    "                    # Track winner by Accuracy\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_model = trained_model\n",
    "                        best_model_name = name\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not results_list: return None, None, None, \"Error: No classification models trained.\"\n",
    "\n",
    "        # Leaderboard & Plot\n",
    "        leaderboard = pd.DataFrame(results_list).sort_values(by=\"Val Accuracy\", ascending=False)\n",
    "        \n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=leaderboard, x='Val Accuracy', y='Model', palette='magma')\n",
    "        plt.title('Classification Leaderboard: Validation Accuracy', fontsize=14)\n",
    "        plt.xlim(0, 1)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Final Test\n",
    "        test_preds = best_model.predict(X_test_final)\n",
    "        test_score = accuracy_score(y_test, test_preds)\n",
    "        test_metric_name = \"Accuracy\"\n",
    "\n",
    "    else:\n",
    "        return None, None, None, f\"Error: Unknown task type '{task_type}'\"\n",
    "\n",
    "    # ==========================\n",
    "    # SHARED FINALIZATION\n",
    "    # ==========================\n",
    "    \n",
    "    # Save Best Model\n",
    "    best_model_path = \"best_model.pkl\"\n",
    "    if best_model:\n",
    "        joblib.dump(best_model, best_model_path)\n",
    "    \n",
    "    status = (f\"‚úÖ Training Complete! Mode: {task_type}\\n\"\n",
    "              f\"üèÜ Best Model: {best_model_name}\\n\"\n",
    "              f\"üìä Val Score:  {best_score:.4f}\\n\"\n",
    "              f\"üöÄ TEST SET {test_metric_name}: {test_score:.4f}\\n\"\n",
    "              f\"üíæ Saved to: {best_model_path}\")\n",
    "    \n",
    "    return leaderboard, fig, best_model_path, status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257cb898",
   "metadata": {},
   "source": [
    "# hyperparameter tunning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a55c1",
   "metadata": {},
   "source": [
    "# grids for regrission and classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72dc56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive hyperparameter grids\n",
    "\n",
    "    #Regression\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200, 300],  # Number of trees\n",
    "        'max_depth': [None, 10, 20, 30],       # Tree depth\n",
    "        'min_samples_split': [2, 5, 10],       # Minimum samples to split\n",
    "        'min_samples_leaf': [1, 2, 4],         # Minimum samples per leaf\n",
    "        'max_features': ['auto', 'sqrt', 'log2']  # Features to consider for splits\n",
    "    },\n",
    "    \n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],        # Number of boosting stages\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Step size shrinkage\n",
    "        'max_depth': [3, 4, 5, 6],             # Maximum depth per tree\n",
    "        'min_samples_split': [2, 5, 10],       # Minimum samples to split\n",
    "        'subsample': [0.8, 0.9, 1.0]           # Fraction of samples for fitting\n",
    "    },\n",
    "    \n",
    "    'Ridge Regression': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],  # Regularization strength\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']  # Algorithm\n",
    "    },\n",
    "    \n",
    "    'Voting Ensemble': {\n",
    "        'ridge__alpha': [0.1, 1.0, 10.0],\n",
    "        'rf__n_estimators': [50, 100],\n",
    "        'rf__max_depth': [10, 20],\n",
    "        'gb__n_estimators': [50, 100],\n",
    "        'gb__learning_rate': [0.05, 0.1]\n",
    "    }\n",
    "} \n",
    "\n",
    "\n",
    "\n",
    "    # 2. Classification Grids (Your New Definitions)\n",
    "param_grids_classification = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        # 'auto' is deprecated in new sklearn, mapping it to 'sqrt' is safer\n",
    "        'max_features': ['sqrt', 'log2'] \n",
    "    },\n",
    "    \n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    \n",
    "    'Logistic Regression': [\n",
    "        # Grid 1: ElasticNet (requires saga)\n",
    "        {\n",
    "            'penalty': ['elasticnet'],\n",
    "            'solver': ['saga'],\n",
    "            'l1_ratio': [0.2, 0.5, 0.8],\n",
    "            'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "        },\n",
    "        # Grid 2: L1/L2 (works with liblinear or saga)\n",
    "        {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "        }\n",
    "    ],\n",
    "    \n",
    "    \"LightGBM\": {\n",
    "        \"num_leaves\": [15, 31, 63],\n",
    "        \"max_depth\": [-1, 5, 10],\n",
    "        \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"subsample\": [0.7, 0.9, 1.0],\n",
    "        \"colsample_bytree\": [0.7, 0.9, 1.0]\n",
    "    },\n",
    "    \n",
    "    \"XGBoost\": {\n",
    "        \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"n_estimators\": [100, 200, 400],\n",
    "        \"subsample\": [0.7, 0.9, 1.0],\n",
    "        \"colsample_bytree\": [0.7, 0.9, 1.0]\n",
    "    },\n",
    "    \n",
    "    'Support Vector Machine': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    \n",
    "    'K-Nearest Neighbors': {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f576986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 10. Hyperparameter Tuning Function ---\n",
    "def tune_models_pipeline(X_train_final, y_train, X_val_final, y_val,X_test_final,y_test, model_name, task_type):\n",
    "    # 1. Safety Checks\n",
    "    if X_train_final is None or y_train is None:\n",
    "        return None, None, None, \"Please run Advanced Processing (Tab 5) first.\"\n",
    "    \n",
    "    # Flatten y for training\n",
    "    y_train = np.ravel(y_train)\n",
    "    y_val = np.ravel(y_val)\n",
    "    y_test = np.ravel(y_test)\n",
    "\n",
    "    # ==========================\n",
    "    # REGRESSION PIPELINE\n",
    "    # ==========================\n",
    "    if task_type == \"Regression\":\n",
    "        \n",
    "        # 2. Retrieve Model and Grid\n",
    "        if model_name not in advanced_models:\n",
    "            return None, None, None, f\"Error: Model '{model_name}' not found in portfolio.\"\n",
    "        \n",
    "        # We need a grid. If not defined, return error.\n",
    "        if model_name not in param_grids:\n",
    "            return None, None, None, f\"Error: No hyperparameter grid defined for '{model_name}'.\"\n",
    "            \n",
    "        base_model = advanced_models[model_name]\n",
    "        grid = param_grids[model_name]\n",
    "        \n",
    "        try:\n",
    "            # 3. Start MLflow Run\n",
    "            with mlflow.start_run(run_name=f\"Tune_{model_name}\"):\n",
    "                \n",
    "                # 4. Optimization (Your Code Logic)\n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=base_model,\n",
    "                    param_distributions=grid,\n",
    "                    n_iter=20,  # Try 20 random combinations\n",
    "                    cv=config.CV_FOLDS,\n",
    "                    scoring='r2',\n",
    "                    n_jobs=config.N_JOBS,\n",
    "                    random_state=config.RANDOM_STATE\n",
    "                    # verbose=1  <-- Removed verbose as it doesn't show in Gradio\n",
    "                )\n",
    "                \n",
    "                # Train\n",
    "                search.fit(X_train_final, y_train)\n",
    "                \n",
    "                # Get Best Results\n",
    "                best_model = search.best_estimator_\n",
    "                best_params = search.best_params_\n",
    "                best_cv_score = search.best_score_\n",
    "                \n",
    "                # 5. Log to MLflow\n",
    "                mlflow.log_params(best_params)\n",
    "                mlflow.log_metric(\"best_cv_r2\", best_cv_score)\n",
    "                mlflow.sklearn.log_model(best_model, \"tuned_model\")\n",
    "                \n",
    "                # 6. Validate on Hold-out Set (Validation Data)\n",
    "                preds = best_model.predict(X_val_final)\n",
    "                val_rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "                val_r2 = r2_score(y_val, preds)\n",
    "                # 6. TEST Scores (Final \"Real World\" Check) <-- NEW STEP\n",
    "                test_preds = best_model.predict(X_test_final)\n",
    "                test_rmse = np.sqrt(mean_squared_error(y_test, test_preds))\n",
    "                test_r2 = r2_score(y_test, test_preds)\n",
    "                \n",
    "                # 7. Create Status Report\n",
    "                status = (f\"‚úÖ Tuning Complete for {model_name}!\\n\"\n",
    "                          f\"-----------------------------------\\n\"\n",
    "                          f\"üèÜ Best CV R¬≤:      {best_cv_score:.4f}\\n\"\n",
    "                          f\"üìä Validation R¬≤:   {val_r2:.4f}\\n\"\n",
    "                          f\"üìâ Validation RMSE: {val_rmse:.4f}\\n\\n\"\n",
    "                          f\"üöÄ FINAL TEST SCORES (Held-out Data):\\n\"\n",
    "                          f\"‚Ä¢ R¬≤ Score: {test_r2:.4f}\\n\"\n",
    "                          f\"‚Ä¢ RMSE:     {test_rmse:.4f}\\n\"\n",
    "                          f\"‚öôÔ∏è Best Parameters:\\n{best_params}\")\n",
    "                \n",
    "                # 8. Plot Performance\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                sns.scatterplot(x=y_val, y=preds, alpha=0.6, ax=ax, color='#2ca02c')\n",
    "                \n",
    "                min_v, max_v = y_val.min(), y_val.max()\n",
    "                ax.plot([min_v, max_v], [min_v, max_v], 'r--', lw=2, label=\"Ideal Fit\")\n",
    "                \n",
    "                ax.set_title(f\"Tuned {model_name} (Val R¬≤: {val_r2:.2f})\")\n",
    "                ax.set_xlabel(\"Actual\")\n",
    "                ax.set_ylabel(\"Predicted\")\n",
    "                ax.legend()\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Return plot as figure object\n",
    "                \n",
    "                # 9. Save Model File\n",
    "                filename = f\"tuned_{model_name.replace(' ', '_')}.pkl\"\n",
    "                joblib.dump(best_model, filename)\n",
    "                \n",
    "                return status, fig, filename, \"Done\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Tuning failed: {str(e)}\", None, None, \"Error\"\n",
    "        \n",
    "\n",
    "    # ==========================\n",
    "    # CLASSIFICATION PIPELINE\n",
    "    # ==========================\n",
    "\n",
    "    elif task_type == \"Classification\":\n",
    "        # Retrieve Model and Grid\n",
    "        if model_name not in classification_models:\n",
    "            return None, None, None, f\"Error: Model '{model_name}' not found in Classification list.\"\n",
    "        if model_name not in param_grids_classification:\n",
    "            return None, None, None, f\"Error: No grid for '{model_name}'.\"\n",
    "\n",
    "        base_model = classification_models[model_name]\n",
    "        grid = param_grids_classification[model_name]\n",
    "        metric = 'accuracy'\n",
    "\n",
    "        try:\n",
    "            with mlflow.start_run(run_name=f\"Tune_{model_name}\"):\n",
    "                # Randomized Search\n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=base_model,\n",
    "                    param_distributions=grid,\n",
    "                    n_iter=20,\n",
    "                    cv=config.CV_FOLDS,\n",
    "                    scoring=metric,\n",
    "                    n_jobs=config.N_JOBS,\n",
    "                    random_state=config.RANDOM_STATE\n",
    "                )\n",
    "                \n",
    "                search.fit(X_train_final, y_train)\n",
    "                \n",
    "                best_model = search.best_estimator_\n",
    "                best_params = search.best_params_\n",
    "                best_cv_score = search.best_score_\n",
    "                \n",
    "                # Scores\n",
    "                val_preds = best_model.predict(X_val_final)\n",
    "                val_score = accuracy_score(y_val, val_preds)\n",
    "                \n",
    "                test_preds = best_model.predict(X_test_final)\n",
    "                test_score = accuracy_score(y_test, test_preds)\n",
    "                \n",
    "                # Log\n",
    "                mlflow.log_params(best_params)\n",
    "                mlflow.log_metric(\"test_accuracy\", test_score)\n",
    "                mlflow.sklearn.log_model(best_model, \"tuned_model\")\n",
    "                \n",
    "                # Status\n",
    "                status = (f\"‚úÖ Tuning Complete for {model_name}!\\n\"\n",
    "                          f\"-----------------------------------\\n\"\n",
    "                          f\"üèÜ Best CV Acc:   {best_cv_score:.2%}\\n\"\n",
    "                          f\"üìä Val Acc:       {val_score:.2%}\\n\"\n",
    "                          f\"üöÄ Test Set Acc:  {test_score:.2%}\\n\\n\"\n",
    "                          f\"‚öôÔ∏è Best Parameters:\\n{best_params}\")\n",
    "                \n",
    "                # Plot: Confusion Matrix\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                cm = confusion_matrix(y_test, test_preds)\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=ax)\n",
    "                ax.set_title(f\"Tuned {model_name} (Test Acc: {test_score:.2%})\")\n",
    "                ax.set_xlabel(\"Predicted\")\n",
    "                ax.set_ylabel(\"Actual\")\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save\n",
    "                filename = f\"tuned_{model_name.replace(' ', '_')}.pkl\"\n",
    "                joblib.dump(best_model, filename)\n",
    "                \n",
    "                return status, fig, filename, \"Done\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Tuning failed: {str(e)}\", None, None, \"Error\"\n",
    "    \n",
    "    return \"Unknown Error\", None, None, \"Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25660135",
   "metadata": {},
   "source": [
    "## üîç THE UI'S SETUP WITH GRADIO AND THE BUTTOM PROSSESORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7eaa9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- The UI (using gr.Blocks) ---\n",
    "with gr.Blocks(title=\"Auto-ML App\") as demo:\n",
    "    \n",
    "    # --- State Components ---\n",
    "    df_state = gr.State()\n",
    "    X_state = gr.State()\n",
    "    y_state = gr.State()\n",
    "    \n",
    "    X_train_processed_state = gr.State()\n",
    "    y_train_state = gr.State()\n",
    "    X_val_processed_state = gr.State()\n",
    "    y_val_state = gr.State()\n",
    "    X_test_processed_state = gr.State()\n",
    "    y_test_state = gr.State()\n",
    "    \n",
    "    # This holds the DataFrame for plotting\n",
    "    X_train_df_state = gr.State()\n",
    "\n",
    "    # ... for getting the type of the dataset form the usee ...\n",
    "    task_type_state = gr.State(value=\"Regression\") # <--- The new memory slot\n",
    "\n",
    "    # States for the FINAL, Scaled Data (after Tab 5)\n",
    "    X_train_final_state = gr.State()\n",
    "    X_val_final_state = gr.State()\n",
    "    X_test_final_state = gr.State()\n",
    "\n",
    "    gr.Markdown(\"<h1 style='text-align: center;'>Automated Model Trainer</h1>\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        \n",
    "        # --- Tab 1: Upload & Split ---\n",
    "        with gr.TabItem(\"1. Upload & Split\"):\n",
    "            gr.Markdown(\"# üìà  Data Splitter\")\n",
    "    \n",
    "            with gr.Row():\n",
    "                file_input = gr.File(label=\"Upload your CSV\", file_types=[\".csv\"])\n",
    "                upload_button = gr.Button(\"1. Upload Data\")\n",
    "            \n",
    "            df_output = gr.DataFrame(label=\"Data Preview \")\n",
    "\n",
    "            gr.Markdown(\"---\")\n",
    "            \n",
    "            y_column_input = gr.Textbox(label=\"Enter Target ('y') Column Name\", placeholder=\"e.g., median_house_value\")\n",
    "            split_button = gr.Button(\"2. Split Data\")\n",
    "            status_output = gr.Textbox(label=\"Status\")\n",
    "\n",
    "            with gr.Row():\n",
    "                X_output = gr.DataFrame(label=\"Features (X)\")\n",
    "                y_output = gr.DataFrame(label=\"Target (y)\")\n",
    "\n",
    "            upload_button.click(fn=load_and_store_data, inputs=[file_input], outputs=[df_output, df_state])\n",
    "            split_button.click(fn=split_and_save_data, inputs=[df_state, y_column_input], outputs=[status_output, X_output, y_output, X_state, y_state])\n",
    "\n",
    "        \n",
    "        # --- Tab 2.5: Advanced Profiling (NEW) ---\n",
    "        with gr.TabItem(\"2.5. Advanced Profiling\"):\n",
    "            gr.Markdown(\"## üìë Comprehensive Data Report\")\n",
    "            gr.Markdown(\"Generate a full automated report (histograms, correlations, warnings) .\")\n",
    "            \n",
    "            report_btn = gr.Button(\"Generate Profile Report\", variant=\"primary\")\n",
    "            \n",
    "            # We use gr.HTML to render the interactive report\n",
    "            report_output = gr.HTML(label=\"Profiling Report\")\n",
    "            \n",
    "            # Connect it to the original dataframe (df_state)\n",
    "            report_btn.click(\n",
    "                fn=generate_profile_report,\n",
    "                inputs=[df_state],\n",
    "                outputs=[report_output]\n",
    "            )\n",
    "            \n",
    "        # --- Tab 2: EDA ---\n",
    "        with gr.TabItem(\"2. EDA\"):\n",
    "            gr.Markdown(\"## Simple Exploratory Data Analysis\")\n",
    "            with gr.Row():\n",
    "                missing_button = gr.Button(\"Find Missing Values\")\n",
    "                categorical_button = gr.Button(\"Find Categorical Columns\")\n",
    "                shape_button = gr.Button(\"Show Data Shape\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                missing_output = gr.DataFrame(label=\"Missing Values\")\n",
    "                categorical_output = gr.DataFrame(label=\"Categorical Columns\")\n",
    "            shape_output = gr.Textbox(label=\"Data Shape\")\n",
    "            \n",
    "            shape_button.click(fn=get_shape, inputs=[df_state], outputs=[shape_output])\n",
    "            missing_button.click(fn=find_missing_values, inputs=[df_state], outputs=[missing_output])\n",
    "            categorical_button.click(fn=find_categorical_columns, inputs=[df_state], outputs=[categorical_output])\n",
    "\n",
    "        # --- Tab 3: Preprocessing ---\n",
    "        with gr.TabItem(\"3. Preprocessing\"):\n",
    "            gr.Markdown(\"Run the full preprocessing pipeline.\")\n",
    "\n",
    "            # --- NEW: Step 1 - The Task Type Selector ---\n",
    "            # We add it here, but we don't change the main function yet.\n",
    "            task_type_radio = gr.Radio(\n",
    "                choices=[\"Regression\", \"Classification\"], \n",
    "                label=\"Select Task Type\", \n",
    "                value=\"Regression\",\n",
    "                info=\"This choice will be used in the next tab for Outlier Analysis.\"\n",
    "            )\n",
    "            # -------------------------------------------\n",
    "            preprocess_button = gr.Button(\"3. Run Preprocessing\")\n",
    "            preprocess_status = gr.Textbox(label=\"Status\", lines=4)\n",
    "            preprocess_preview = gr.DataFrame(label=\"Processed X_train Preview\")\n",
    "            \n",
    "            # --- FIXED OUTPUTS: Added X_train_df_state as 9th output ---\n",
    "\n",
    "            # --- 1. Connect the Radio Button to the State ---\n",
    "            # This updates 'task_type_state' instantly whenever the user changes the radio button\n",
    "            task_type_radio.change(\n",
    "                fn=update_task_type, \n",
    "                inputs=[task_type_radio], \n",
    "                outputs=[task_type_state] # <--- Saved to memory!\n",
    "            )\n",
    "\n",
    "            \n",
    "            preprocess_button.click(\n",
    "                fn=preprocess_data,\n",
    "                inputs=[X_state, y_state],\n",
    "                outputs=[\n",
    "                    preprocess_status, \n",
    "                    preprocess_preview,\n",
    "                    X_train_processed_state, y_train_state,\n",
    "                    X_val_processed_state, y_val_state,\n",
    "                    X_test_processed_state, y_test_state,\n",
    "                    X_train_df_state # <--- This is the critical missing piece!\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # --- Tab 4: Model EDA (ADDED) ---\n",
    "        with gr.TabItem(\"4. Model EDA\"):\n",
    "            gr.Markdown(\"Explore the processed training data.\")\n",
    "            with gr.Row():\n",
    "                plot_dists_btn = gr.Button(\"Plot All Distributions\")\n",
    "                plot_corr_btn = gr.Button(\"Generate Correlation Heatmap\")\n",
    "            \n",
    "            plot_output = gr.Plot(label=\"Plot Output\")\n",
    "            \n",
    "            plot_dists_btn.click(fn=plot_all_distributions, inputs=[X_train_df_state], outputs=[plot_output])\n",
    "            plot_corr_btn.click(fn=plot_correlation, inputs=[X_train_df_state], outputs=[plot_output])\n",
    "\n",
    "        # --- Tab 5: Advanced Processing (NEW) ---\n",
    "        with gr.TabItem(\"5. Advanced Processing\"):\n",
    "            gr.Markdown(\"## üöÄ Stage 2: Advanced Transformation\")\n",
    "            gr.Markdown(\"Apply **Outlier Capping** and **Robust Scaling**\")\n",
    "            \n",
    "            advanced_button = gr.Button(\"Run Advanced Pipeline\")\n",
    "            advanced_status = gr.Textbox(label=\"Status\", lines=4)\n",
    "            \n",
    "            # Inputs: The arrays from Stage 1 + The Task Type\n",
    "            # Outputs: The final arrays + Status box\n",
    "            advanced_button.click(\n",
    "                fn=scale_and_clean_data,\n",
    "                inputs=[X_train_processed_state, X_val_processed_state, X_test_processed_state, task_type_state],\n",
    "                outputs=[advanced_status, X_train_final_state, X_val_final_state, X_test_final_state]\n",
    "\n",
    "            )\n",
    "\n",
    "            # --- Tab 6: Auto-ML Training ---\n",
    "        with gr.TabItem(\"6. Train Model\"):\n",
    "            gr.Markdown(\"## ü§ñ Auto-ML Training\")\n",
    "            gr.Markdown(\"Train the entire portfolio of models using your **Cleaned & Scaled Data**.\")\n",
    "            \n",
    "            train_all_btn = gr.Button(\"üöÄ Run Auto-ML Experiment\", variant=\"primary\")\n",
    "            train_status = gr.Textbox(label=\"Experiment Status\", lines=8, max_lines=20, interactive=False)\n",
    "            \n",
    "            with gr.Row():\n",
    "                leaderboard_output = gr.DataFrame(label=\"üèÜ Model Leaderboard\")\n",
    "                model_file_output = gr.File(label=\"üíæ Download Best Model\")\n",
    "            \n",
    "            comparison_plot = gr.Plot(label=\"üìä R¬≤ Score Comparison\")\n",
    "            \n",
    "            train_all_btn.click(\n",
    "                fn=train_models_pipeline,\n",
    "                inputs=[\n",
    "                    X_train_final_state, # <--- The Final Clean Data\n",
    "                    y_train_state, \n",
    "                    X_val_final_state,   # <--- The Final Clean Data\n",
    "                    y_val_state,\n",
    "                    X_test_final_state, # <--- Added Input\n",
    "                    y_test_state,       # <--- Added Input \n",
    "                    task_type_state\n",
    "                ],\n",
    "                outputs=[leaderboard_output, comparison_plot, model_file_output, train_status]\n",
    "            )\n",
    "\n",
    "        # --- Tab 7: Hyperparameter Tuning ---\n",
    "        with gr.TabItem(\"7. Tune Model\"):\n",
    "            gr.Markdown(\"## üéõÔ∏è Hyperparameter Tuning\")\n",
    "            gr.Markdown(\"Optimize a specific model to find the best settings using **RandomizedSearchCV**.\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                # Dropdown: Only shows models that we have grids for\n",
    "                tune_model_selector = gr.Dropdown(\n",
    "                    choices=list(param_grids.keys()), \n",
    "                    label=\"Select Model to Tune\",\n",
    "                    value=\"Random Forest\"\n",
    "                )\n",
    "                tune_btn = gr.Button(\"Start Tuning\", variant=\"primary\")\n",
    "            \n",
    "            tune_status = gr.Textbox(label=\"Tuning Results\", lines=10)\n",
    "            \n",
    "            with gr.Row():\n",
    "                tune_plot = gr.Plot(label=\"Tuned Model Performance\")\n",
    "                tuned_model_file = gr.File(label=\"Download Tuned Model\")\n",
    "            \n",
    "            tune_btn.click(\n",
    "                fn=tune_models_pipeline,\n",
    "                inputs=[\n",
    "                    X_train_final_state, \n",
    "                    y_train_state, \n",
    "                    X_val_final_state, \n",
    "                    y_val_state, \n",
    "                    X_test_final_state, # <--- Added Input\n",
    "                    y_test_state,\n",
    "                    tune_model_selector,\n",
    "                    task_type_state\n",
    "                ],\n",
    "                outputs=[tune_status, tune_plot, tuned_model_file]\n",
    "            )    \n",
    "      \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684beb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49c91c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAIR_2_project (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
